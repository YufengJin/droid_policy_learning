"""
RLDS training entry point using Hydra for config.

Usage:
  # Default Hydra config (train_configs/train_rlds.yaml):
  python -m robomimic.scripts.train_rlds

  # Override from CLI:
  python -m robomimic.scripts.train_rlds train.data_path=/workspace/dataset \
      train.dataset_names=[role_ros2] experiment.name=my_exp

  # Load a JSON generated by config_gen (droid_runs_language_conditioned_rlds):
  python -m robomimic.scripts.train_rlds load_from=/path/to/generated.json

  # Debug mode (short run):
  python -m robomimic.scripts.train_rlds debug=true

Workflow with config_gen:
  1. Generate configs: python -m robomimic.scripts.config_gen.droid_runs_language_conditioned_rlds \\
        --name my_run --env droid --mod im
  2. Run with a generated JSON: train_rlds load_from=<path-to-generated>/<exp_name>.json
  Or run with train.py: python -m robomimic.scripts.train --config <path-to-generated>.json
"""

import json
import os
import sys
import traceback
import datetime

# Add robomimic package to path when running as script
_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
_ROBOMIMIC_ROOT = os.path.abspath(os.path.join(_SCRIPT_DIR, os.pardir, os.pardir))
if _ROBOMIMIC_ROOT not in sys.path:
    sys.path.insert(0, _ROBOMIMIC_ROOT)

from omegaconf import OmegaConf


def _get_config_path():
    return os.path.join(_SCRIPT_DIR, "train_configs")


def run(cfg: "OmegaConf") -> str:
    from robomimic.config import config_factory
    from robomimic.utils import torch_utils as TorchUtils
    from robomimic.scripts.train import train

    # Optional: load full config from a JSON (e.g. from config_gen)
    load_from = OmegaConf.select(cfg, "load_from")
    if load_from is not None and str(load_from).strip() != "":
        with open(os.path.expanduser(load_from), "r") as f:
            cfg = OmegaConf.create(json.load(f))

    # Extract and remove Hydra-only keys before building robomimic config
    debug = OmegaConf.select(cfg, "debug")
    if debug is None:
        debug = False

    cfg_dict = OmegaConf.to_container(cfg, resolve=True)
    if not isinstance(cfg_dict, dict):
        raise TypeError("Resolved config is not a dict")
    cfg_dict.pop("load_from", None)
    cfg_dict.pop("debug", None)  # Hydra-only: 已用 OmegaConf 读出，不写入 robomimic config（config 无此 key）

    # Build robomimic config
    config = config_factory(cfg_dict["algo_name"])
    with config.values_unlocked():
        config.update(cfg_dict)
    
    # Auto-generate experiment name if not provided
    exp_name = config.experiment.name
    if exp_name is None or exp_name == "null" or (isinstance(exp_name, str) and exp_name.strip() == ""):
        # Generate experiment name based on algo_name, dataset_names, and timestamp
        algo_name = config.algo_name
        
        # Get dataset names
        dataset_names = config.train.dataset_names
        if isinstance(dataset_names, list) and len(dataset_names) > 0:
            # Use first dataset name, or combine if multiple
            if len(dataset_names) == 1:
                dataset_str = dataset_names[0]
            else:
                dataset_str = "_".join(dataset_names[:2])  # Use first 2 datasets
                if len(dataset_names) > 2:
                    dataset_str += f"_plus{len(dataset_names)-2}"
        else:
            dataset_str = "unknown_dataset"
        
        # Add timestamp for uniqueness (format: YYYYMMDD_HHMMSS)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Generate name: algo_dataset_timestamp
        auto_name = f"{algo_name}_{dataset_str}_{timestamp}"
        config.experiment.name = auto_name
        print(f"[INFO] Auto-generated experiment name: {auto_name}")
    else:
        print(f"[INFO] Using provided experiment name: {exp_name}")

    device = TorchUtils.get_torch_device(try_to_use_cuda=config.train.cuda)

    if debug:
        config.unlock()
        config.lock_keys()
        config.experiment.epoch_every_n_steps = 3
        config.experiment.validation_epoch_every_n_steps = 3
        config.train.num_epochs = 200
        config.experiment.mse.every_n_epochs = 2
        config.experiment.save.every_n_epochs = 1
        config.experiment.rollout.rate = 1
        config.experiment.rollout.n = 2
        config.experiment.rollout.horizon = 10
        config.train.output_dir = "/tmp/tmp_trained_models"
        # 关闭实验日志（TensorBoard、WandB、终端输出到文件）
        config.experiment.logging.terminal_output_to_txt = True
        config.experiment.logging.log_tb = False
        config.experiment.logging.log_wandb = False

    config.lock()

    train(config, device=device, debug=debug)
    return "finished run successfully!"


def main() -> None:
    from hydra import compose, initialize_config_dir
    from hydra.core.global_hydra import GlobalHydra

    config_dir = os.path.abspath(_get_config_path())
    if not os.path.isdir(config_dir):
        raise FileNotFoundError(f"Config directory not found: {config_dir}")

    # Compose config from train_configs/train_rlds.yaml; CLI overrides from sys.argv
    GlobalHydra.instance().clear()
    overrides = list(sys.argv[1:])
    with initialize_config_dir(config_dir=config_dir, version_base="1.1"):
        cfg = compose(config_name="train_rlds", overrides=overrides)

    res_str = "finished run successfully!"
    try:
        res_str = run(cfg)
    except Exception as e:
        res_str = "run failed with error:\n{}\n\n{}".format(e, traceback.format_exc())
    print(res_str)


if __name__ == "__main__":
    main()
