# ============================================================================
# Hydra 配置文件：ALOHA HDF5 数据格式训练配置
# ============================================================================
# 使用方法：
#   1. 多卡 DDP 训练:
#      torchrun --nproc_per_node=4 -m robomimic.scripts.train_aloha
#   2. 带覆盖:
#      torchrun --nproc_per_node=8 -m robomimic.scripts.train_aloha \
#          'train.data=[{path: /workspace/datasets/aloha/}]' \
#          train.batch_size=32
#   3. 单卡训练:
#      python -m robomimic.scripts.train_aloha
#   4. Debug 模式:
#      python -m robomimic.scripts.train_aloha debug=true
# ============================================================================

load_from: null
debug: false

# ----------------------------------------------------------------------------
# 算法配置
# ----------------------------------------------------------------------------
algo_name: diffusion_policy

# ----------------------------------------------------------------------------
# 实验配置 (experiment)
# ----------------------------------------------------------------------------
experiment:
  name: null  # 自动生成

  validate: false  # ALOHA 默认不做验证（需要 train/val 子目录）

  logging:
    terminal_output_to_txt: true
    log_tb: true
    log_wandb: true
    wandb_proj_name: diffusion_policy_aloha

  mse:
    enabled: true
    every_n_epochs: 10
    on_save_ckpt: true
    num_samples: 6
    visualize: true

  save:
    enabled: true
    every_n_seconds: null
    every_n_epochs: 50
    epochs: []
    on_best_validation: false
    on_best_rollout_return: false
    on_best_rollout_success_rate: true

  epoch_every_n_steps: 100
  validation_epoch_every_n_steps: 10

  env: null
  additional_envs: null
  ckpt_path: null   # 设为已有 .pth 可加载权重
  resume: false     # true=完整恢复(optimizer+scheduler+epoch)；false=仅加载权重(微调)
  env_meta_update_dict: {}

  render: false
  render_video: true
  keep_all_videos: false
  video_skip: 5

  rollout:
    enabled: false
    n: 50
    horizon: 400
    rate: 50
    warmstart: 0
    terminate_on_success: true

# ----------------------------------------------------------------------------
# 训练配置 (train)
# ----------------------------------------------------------------------------
train:
  # ALOHA 数据集路径（目录，内含多个 HDF5 文件，每个文件对应一个 episode）
  # 目录中可包含 train/ 和 val/ 子目录
  data:
    - path: /workspace/datasets/aloha/
      weight: 1.0

  data_format: robomimic

  output_dir: /workspace/droid_policy_learning/outputs

  # HDF5 数据集参数（ALOHA 使用目录扫描，filter_key 不适用）
  hdf5_filter_key: null
  hdf5_validation_filter_key: null
  hdf5_cache_mode: low_dim
  hdf5_use_swmr: true
  hdf5_normalize_obs: false
  hdf5_load_next_obs: false
  num_data_workers: 4

  # 序列配置
  seq_length: 25      # ALOHA 通常使用较长的 chunk size
  pad_seq_length: true
  frame_stack: 2
  pad_frame_stack: true

  dataset_keys:
    - actions
  goal_mode: null
  truncated_geom_factor: 0.3

  # 训练超参数
  cuda: true
  batch_size: 32       # ALOHA 图像较大，batch_size 适当降低
  num_epochs: 100000
  seed: 1

  # ALOHA 动作配置: 14D (双臂各 7D: 6D joint + 1D gripper)
  action_keys:
    - actions

  action_shapes:
    - [1, 14]

  action_config:
    actions: {normalization: min_max}

  shuffled_obs_key_groups: []

# ----------------------------------------------------------------------------
# 算法特定配置 (algo) — Diffusion Policy
# ----------------------------------------------------------------------------
algo:
  optim_params:
    policy:
      learning_rate:
        initial: 0.0001
        decay_factor: 0.1
        epoch_schedule: []
      regularization: {L2: 0.0}

  horizon:
    observation_horizon: 2
    action_horizon: 8
    prediction_horizon: 25   # 匹配 ALOHA 的 chunk_size

  unet:
    enabled: true
    diffusion_step_embed_dim: 256
    down_dims: [256, 512, 1024]
    kernel_size: 5
    n_groups: 8

  ema:
    enabled: true
    power: 0.75

  ddpm:
    enabled: false
    num_train_timesteps: 100
    num_inference_timesteps: 100
    beta_schedule: squaredcos_cap_v2
    clip_sample: true
    prediction_type: epsilon

  ddim:
    enabled: true
    num_train_timesteps: 100
    num_inference_timesteps: 10
    beta_schedule: squaredcos_cap_v2
    clip_sample: true
    set_alpha_to_one: true
    steps_offset: 0
    prediction_type: epsilon

  noise_samples: 8

# ----------------------------------------------------------------------------
# 观察配置 (observation)
# ALOHA: 3 cameras (cam_high + 2 wrist), 14D proprio (qpos)
# ----------------------------------------------------------------------------
observation:
  image_dim: [128, 128]

  modalities:
    obs:
      low_dim:
        - qpos                          # 14D 关节位置 (双臂)
      rgb:
        - cam_high_image                # 主相机 (映射到 observations/images/cam_high)
        - cam_left_wrist_image          # 左手腕相机
        - cam_right_wrist_image         # 右手腕相机
      depth: []
      scan: []
    goal:
      low_dim: []
      rgb: []
      depth: []
      scan: []

  encoder:
    low_dim:
      core_class: null
      core_kwargs: {}
      obs_randomizer_class: null
      obs_randomizer_kwargs: {}

    rgb:
      core_class: VisualCore
      fuser: null
      core_kwargs:
        feature_dimension: 512
        flatten: true
        backbone_class: ResNet50Conv
        backbone_kwargs:
          pretrained: true
          use_cam: false
          downsample: false
        pool_class: null
        pool_kwargs: null
      obs_randomizer_class:
        - ColorRandomizer
        - CropRandomizer
      obs_randomizer_kwargs:
        - {}
        - {crop_height: 116, crop_width: 116, num_crops: 1, pos_enc: false}

    depth:
      core_class: VisualCore
      core_kwargs: {}
      obs_randomizer_class: null
      obs_randomizer_kwargs: {}

    scan:
      core_class: ScanCore
      core_kwargs: {}
      obs_randomizer_class: null
      obs_randomizer_kwargs: {}
