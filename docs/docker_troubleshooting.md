# Docker æ•…éšœæ’é™¤æŒ‡å—

æœ¬æ–‡æ¡£æ¶µç›–äº†ä½¿ç”¨ Docker è¿›è¡Œ DROID ç­–ç•¥è®­ç»ƒæ—¶å¯èƒ½é‡åˆ°çš„å¸¸è§é—®é¢˜å’Œè­¦å‘Šã€‚

## ç›®å½•

- [GPU è®¿é—®é—®é¢˜](#gpu-è®¿é—®é—®é¢˜)
- [TensorFlow è­¦å‘Š](#tensorflow-è­¦å‘Š)
- [CUDA ç‰ˆæœ¬å…¼å®¹æ€§](#cuda-ç‰ˆæœ¬å…¼å®¹æ€§)
- [å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ](#å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ)

---

## GPU è®¿é—®é—®é¢˜

### æ£€æŸ¥ GPU æ˜¯å¦å¯è®¿é—®

åœ¨å®¹å™¨å†…è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ GPU çŠ¶æ€ï¼š

```bash
# æ£€æŸ¥ nvidia-smi
docker exec <container_name> nvidia-smi

# æ£€æŸ¥ PyTorch GPU è®¿é—®
docker exec <container_name> micromamba run -n droid_env python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"
```

### é¢„æœŸè¾“å‡º

å¦‚æœé…ç½®æ­£ç¡®ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
- `nvidia-smi` æ˜¾ç¤º GPU ä¿¡æ¯
- PyTorch æŠ¥å‘Š `CUDA available: True` å’Œ `GPU count: 1`ï¼ˆæˆ–æ›´å¤šï¼‰

### å¦‚æœ GPU ä¸å¯è®¿é—®

1. **æ£€æŸ¥ Docker æ˜¯å¦æ”¯æŒ NVIDIA è¿è¡Œæ—¶**ï¼š
   ```bash
   docker info | grep -i runtime
   ```
   åº”è¯¥çœ‹åˆ° `nvidia` åœ¨è¿è¡Œæ—¶åˆ—è¡¨ä¸­ã€‚

2. **æ£€æŸ¥ nvidia-container-toolkit**ï¼š
   ```bash
   which nvidia-container-runtime
   ```

3. **éªŒè¯ docker-compose é…ç½®**ï¼š
   ç¡®ä¿ `docker-compose.yaml` ä¸­åŒ…å«æ­£ç¡®çš„ GPU é…ç½®ï¼š
   ```yaml
   deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             count: all
             capabilities: [gpu, compute, utility]
   ```

4. **é‡å¯ Docker æœåŠ¡**ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼š
   ```bash
   sudo systemctl restart docker
   ```

---

## TensorFlow è­¦å‘Š

### å¸¸è§è­¦å‘Šä¿¡æ¯

åœ¨è®­ç»ƒå¯åŠ¨æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šçœ‹åˆ°ä»¥ä¸‹ TensorFlow è­¦å‘Šï¼š

```
Could not find cuda drivers on your machine, GPU will not be used.
Cannot dlopen some GPU libraries.
Unable to register cuDNN/cuFFT/cuBLAS factory
TF-TRT Warning: Could not find TensorRT
```

### æ˜¯å¦éœ€è¦è§£å†³ï¼Ÿ

**ç­”æ¡ˆï¼šé€šå¸¸ä¸éœ€è¦ç«‹å³è§£å†³**

#### åŸå› åˆ†æ

1. **è®­ç»ƒä½¿ç”¨ PyTorchï¼Œä¸æ˜¯ TensorFlow**
   - DROID ç­–ç•¥è®­ç»ƒå®Œå…¨åŸºäº PyTorchï¼ˆ`diffusion_policy` ç®—æ³•ï¼‰
   - TensorFlow ä»…ä½œä¸º Octo åº“çš„ä¾èµ–é¡¹å®‰è£…ï¼Œè®­ç»ƒæµç¨‹ä¸ç›´æ¥ä½¿ç”¨

2. **PyTorch GPU æ­£å¸¸å·¥ä½œ**
   - å·²éªŒè¯ PyTorch å¯ä»¥æ­£å¸¸è®¿é—® GPU
   - è®­ç»ƒä¼šåœ¨ GPU ä¸Šæ‰§è¡Œï¼Œæ€§èƒ½ä¸å—å½±å“

3. **è­¦å‘Šå½±å“è¯„ä¼°**

| è­¦å‘Šç±»å‹ | ä¸¥é‡æ€§ | æ˜¯å¦å½±å“è®­ç»ƒ | æ˜¯å¦éœ€è¦ä¿®å¤ |
|---------|--------|------------|------------|
| `Could not find cuda drivers` (TensorFlow) | ä½ | å¦ | å¦ |
| `Cannot dlopen GPU libraries` (TensorFlow) | ä½ | å¦ | å¦ |
| `TF-TRT Warning` | ä½ | å¦ | å¦ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰ |
| cuDNN/cuFFT/cuBLAS æ³¨å†Œé”™è¯¯ | ä½ | å¦ | å¦ |

### æ ¹æœ¬åŸå› 

- **CUDA ç‰ˆæœ¬ä¸åŒ¹é…**ï¼š
  - å®¹å™¨ä½¿ç”¨ CUDA 11.8ï¼ˆä¸ PyTorch 2.0.1 å…¼å®¹ï¼‰
  - TensorFlow 2.15.0 éœ€è¦ CUDA 12.x
  - è¿™å¯¼è‡´ TensorFlow æ— æ³•è®¿é—® GPUï¼Œä½† PyTorch æ­£å¸¸å·¥ä½œ

### å¦‚æœå°†æ¥éœ€è¦ä¿®å¤ï¼ˆå¯é€‰ï¼‰

å¦‚æœå°†æ¥éœ€è¦ä½¿ç”¨ Octo çš„ TensorFlow GPU åŠŸèƒ½ï¼Œå¯ä»¥è€ƒè™‘ï¼š

#### é€‰é¡¹ 1ï¼šå‡çº§åˆ° CUDA 12.x

ä¿®æ”¹ `docker/Dockerfile`ï¼š
```dockerfile
FROM nvidia/cuda:12.4.0-cudnn8-devel-ubuntu22.04
# æ›´æ–° PyTorch å®‰è£…å‘½ä»¤ä»¥åŒ¹é… CUDA 12.x
RUN pip install torch==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121
```

#### é€‰é¡¹ 2ï¼šé™çº§ TensorFlow

ä¿®æ”¹ `docker/Dockerfile`ï¼š
```dockerfile
# ä½¿ç”¨æ”¯æŒ CUDA 11.8 çš„ TensorFlow ç‰ˆæœ¬
RUN pip install "tensorflow==2.13.0"
```

**æ³¨æ„**ï¼šè¿™äº›æ›´æ”¹å¯èƒ½ä¼šå½±å“å…¶ä»–ä¾èµ–é¡¹çš„å…¼å®¹æ€§ï¼Œè¯·è°¨æ…æ“ä½œã€‚

---

## CUDA ç‰ˆæœ¬å…¼å®¹æ€§

### å½“å‰é…ç½®

- **ä¸»æœº CUDA ç‰ˆæœ¬**ï¼š12.4ï¼ˆé€šè¿‡ `nvidia-smi` æŸ¥çœ‹ï¼‰
- **å®¹å™¨ CUDA ç‰ˆæœ¬**ï¼š11.8ï¼ˆåœ¨ Dockerfile ä¸­æŒ‡å®šï¼‰
- **PyTorch CUDA ç‰ˆæœ¬**ï¼š11.8ï¼ˆä¸å®¹å™¨åŒ¹é…ï¼‰
- **TensorFlow CUDA è¦æ±‚**ï¼š12.xï¼ˆä¸åŒ¹é…ï¼Œå¯¼è‡´è­¦å‘Šï¼‰

### ç‰ˆæœ¬å…¼å®¹æ€§è¡¨

| ç»„ä»¶ | ç‰ˆæœ¬ | CUDA è¦æ±‚ | çŠ¶æ€ |
|------|------|-----------|------|
| PyTorch | 2.0.1+cu118 | CUDA 11.8 | âœ… å…¼å®¹ |
| TensorFlow | 2.15.0 | CUDA 12.x | âš ï¸ ä¸å…¼å®¹ï¼ˆä½†ä¸å½±å“è®­ç»ƒï¼‰ |
| å®¹å™¨åŸºç¡€é•œåƒ | CUDA 11.8 | CUDA 11.8 | âœ… å…¼å®¹ |
| ä¸»æœºé©±åŠ¨ | 550.54.15 | CUDA 12.4 | âœ… å‘åå…¼å®¹ |

### ä¸ºä»€ä¹ˆå¯ä»¥å·¥ä½œï¼Ÿ

- NVIDIA é©±åŠ¨ï¼ˆ550.54.15ï¼‰å‘åå…¼å®¹ CUDA 11.8 å’Œ 12.4
- PyTorch ä½¿ç”¨å®¹å™¨å†…çš„ CUDA 11.8 åº“ï¼Œä¸é©±åŠ¨å…¼å®¹
- TensorFlow æ— æ³•æ‰¾åˆ°åŒ¹é…çš„ CUDA åº“ï¼Œä½†è®­ç»ƒä¸ä½¿ç”¨ TensorFlow

---

## å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### é”™è¯¯ 1ï¼šå®¹å™¨å¯åŠ¨å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
Error response from daemon: could not select device driver "" with capabilities: [[gpu]]
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿å®‰è£…äº† `nvidia-container-toolkit`ï¼š
   ```bash
   sudo apt-get update
   sudo apt-get install -y nvidia-container-toolkit
   sudo systemctl restart docker
   ```

2. éªŒè¯ Docker é…ç½®ï¼š
   ```bash
   cat /etc/docker/daemon.json
   ```
   åº”è¯¥åŒ…å«ï¼š
   ```json
   {
     "runtimes": {
       "nvidia": {
         "path": "nvidia-container-runtime",
         "runtimeArgs": []
       }
     }
   }
   ```

### é”™è¯¯ 2ï¼šå…±äº«å†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼š
```
RuntimeError: DataLoader worker (pid XXXX) is killed by signal: Bus error
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
ç¡®ä¿ `docker-compose.yaml` ä¸­è®¾ç½®äº† `ipc: host`ï¼š
```yaml
ipc: host
```

å¦‚æœæ— æ³•ä½¿ç”¨ `ipc: host`ï¼Œå¢åŠ å…±äº«å†…å­˜å¤§å°ï¼š
```yaml
shm_size: 32gb
```

### é”™è¯¯ 3ï¼šå†…å­˜é”å®šå¤±è´¥

**ç—‡çŠ¶**ï¼š
```
CUDA error: out of memory
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
ç¡®ä¿ `docker-compose.yaml` ä¸­è®¾ç½®äº†ï¼š
```yaml
ulimits:
  memlock: -1
```

### é”™è¯¯ 4ï¼šæ–‡ä»¶æè¿°ç¬¦é™åˆ¶

**ç—‡çŠ¶**ï¼š
```
OSError: [Errno 24] Too many open files
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
åœ¨ `docker-compose.yaml` ä¸­å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶ï¼š
```yaml
ulimits:
  nofile:
    soft: 65536
    hard: 65536
```

---

## éªŒè¯é…ç½®

è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ Docker é…ç½®æ˜¯å¦æ­£ç¡®ï¼š

```bash
# 1. æ£€æŸ¥ GPU è®¿é—®
docker exec <container_name> nvidia-smi

# 2. æ£€æŸ¥ PyTorch
docker exec <container_name> micromamba run -n droid_env python -c "import torch; assert torch.cuda.is_available(), 'GPU not available!'; print('âœ… PyTorch GPU: OK')"

# 3. æ£€æŸ¥ CUDA ç‰ˆæœ¬
docker exec <container_name> micromamba run -n droid_env python -c "import torch; print(f'PyTorch CUDA: {torch.version.cuda}')"

# 4. æ£€æŸ¥å…±äº«å†…å­˜ï¼ˆå¦‚æœä½¿ç”¨ ipc: hostï¼Œåº”è¯¥æ˜¾ç¤ºä¸»æœºå†…å­˜ï¼‰
docker exec <container_name> df -h /dev/shm
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ä½¿ç”¨ Host ç½‘ç»œæ¨¡å¼

```yaml
network_mode: host
```

**ä¼˜åŠ¿**ï¼š
- æ¶ˆé™¤ Docker ç½‘æ¡¥å¼€é”€
- å¯¹ NCCL åˆ†å¸ƒå¼è®­ç»ƒè‡³å…³é‡è¦
- ä½å»¶è¿Ÿ API æœåŠ¡

### 2. ä½¿ç”¨ Host IPC æ¨¡å¼

```yaml
ipc: host
```

**ä¼˜åŠ¿**ï¼š
- æ— é™åˆ¶å…±äº«å†…å­˜è®¿é—®
- PyTorch DataLoader æ€§èƒ½æœ€ä½³
- é¿å…å…±äº«å†…å­˜ä¸è¶³é—®é¢˜

### 3. ä½¿ç”¨ Host PID æ¨¡å¼

```yaml
pid: host
```

**ä¼˜åŠ¿**ï¼š
- ç›‘æ§å·¥å…·ï¼ˆnvtop, htop, wandbï¼‰å¯ä»¥æ­£ç¡®æ˜¾ç¤ºç³»ç»Ÿèµ„æº
- ä¾¿äºè°ƒè¯•å’Œæ€§èƒ½åˆ†æ

### 4. æŒ‚è½½ Hugging Face Cache

```yaml
volumes:
  - ${HOME}/.cache/huggingface:/root/.cache/huggingface
```

**ä¼˜åŠ¿**ï¼š
- é¿å…é‡å¤ä¸‹è½½æ¨¡å‹
- èŠ‚çœæ—¶é—´å’Œå¸¦å®½

---

## è·å–å¸®åŠ©

å¦‚æœé‡åˆ°å…¶ä»–é—®é¢˜ï¼š

1. æ£€æŸ¥ [Docker å®˜æ–¹æ–‡æ¡£](https://docs.docker.com/)
2. æŸ¥çœ‹ [NVIDIA Container Toolkit æ–‡æ¡£](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
3. æ£€æŸ¥é¡¹ç›® [GitHub Issues](https://github.com/droid-dataset/droid_policy_learning/issues)

---

## æ€»ç»“

- âœ… **PyTorch GPU è®¿é—®æ­£å¸¸**ï¼šè®­ç»ƒä¼šåœ¨ GPU ä¸Šæ‰§è¡Œ
- âš ï¸ **TensorFlow è­¦å‘Šå¯ä»¥å¿½ç•¥**ï¼šä¸å½±å“è®­ç»ƒæ€§èƒ½
- âœ… **å½“å‰é…ç½®å·²ä¼˜åŒ–**ï¼šéµå¾ª HPC/LLM è®­ç»ƒæœ€ä½³å®è·µ
- ğŸ“ **æ–‡æ¡£å·²æ›´æ–°**ï¼šåŒ…å«æ‰€æœ‰å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

å¦‚æœè®­ç»ƒæ­£å¸¸è¿è¡Œä¸” PyTorch å¯ä»¥è®¿é—® GPUï¼Œæ‚¨å¯ä»¥å®‰å…¨åœ°å¿½ç•¥ TensorFlow ç›¸å…³çš„è­¦å‘Šã€‚
