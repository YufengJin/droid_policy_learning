# å¤š GPU è®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•åœ¨ DROID Policy Learning ä¸­ä½¿ç”¨å¤šä¸ª GPU è¿›è¡Œè®­ç»ƒï¼Œå¹¶è¯¦ç»†è¯´æ˜ `DistributedDataParallel` å’Œ `DataParallel` çš„åŒºåˆ«ã€‚

## ç›®å½•

- [å¿«é€Ÿå¼€å§‹ï¼šç›´æ¥è¿è¡Œå¤šå¡è®­ç»ƒ](#å¿«é€Ÿå¼€å§‹ç›´æ¥è¿è¡Œå¤šå¡è®­ç»ƒ)
- [DataParallel vs DistributedDataParallel](#dataparallel-vs-distributeddataparallel)
- [å½“å‰å®ç°çŠ¶æ€](#å½“å‰å®ç°çŠ¶æ€)
- [æ–¹æ³• 1: DataParallelï¼ˆç®€å•ä½†æ•ˆç‡è¾ƒä½ï¼‰](#æ–¹æ³•-1-dataparallelç®€å•ä½†æ•ˆç‡è¾ƒä½)
- [æ–¹æ³• 2: DistributedDataParallelï¼ˆæ¨èï¼‰](#æ–¹æ³•-2-distributeddataparallelæ¨è)
- [æ–¹æ³• 3: ä½¿ç”¨ torchrunï¼ˆæœ€ç®€å•ï¼‰](#æ–¹æ³•-3-ä½¿ç”¨-torchrunæœ€ç®€å•)
- [é…ç½®å»ºè®®](#é…ç½®å»ºè®®)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## å¿«é€Ÿå¼€å§‹ï¼šç›´æ¥è¿è¡Œå¤šå¡è®­ç»ƒ

### æœ€ç®€å•çš„æ–¹å¼ï¼ˆæ¨èï¼‰

**æ— éœ€ä»»ä½•ä¿®æ”¹ï¼Œç›´æ¥è¿è¡Œè®­ç»ƒå‘½ä»¤å³å¯ä½¿ç”¨å¤šå¡ï¼**

```bash
# ç›´æ¥è¿è¡Œï¼Œè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ GPU
python -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json
```

ä»£ç ä¼šè‡ªåŠ¨ï¼š
- âœ… æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„ GPU
- âœ… è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ GPU è¿›è¡Œè®­ç»ƒï¼ˆé€šè¿‡ DataParallelï¼‰
- âœ… æ— éœ€ä»»ä½•é¢å¤–é…ç½®æˆ–ä¿®æ”¹

### éªŒè¯å¤šå¡æ˜¯å¦åœ¨ä½¿ç”¨

```bash
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œï¼Œå®æ—¶ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi
```

å¦‚æœçœ‹åˆ°æ‰€æœ‰ GPU éƒ½åœ¨ä½¿ç”¨ï¼ˆä½¿ç”¨ç‡ > 0%ï¼‰ï¼Œè¯´æ˜å¤šå¡è®­ç»ƒå·²å¯ç”¨ã€‚

### æŒ‡å®šä½¿ç”¨çš„ GPU

```bash
# åªä½¿ç”¨ GPU 0 å’Œ GPU 1
CUDA_VISIBLE_DEVICES=0,1 python -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json

# åªä½¿ç”¨ GPU 2 å’Œ GPU 3
CUDA_VISIBLE_DEVICES=2,3 python -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json
```

### æ€§èƒ½è¯´æ˜

- **DataParallel**ï¼šå½“å‰å®ç°ä½¿ç”¨è¿™ç§æ–¹å¼ï¼Œé€‚åˆ 2-4 GPU
- **æ€§èƒ½**ï¼šæ¯”å• GPU å¿«ï¼Œä½†ä¸å¦‚ DDP é«˜æ•ˆ
- **é€‚ç”¨åœºæ™¯**ï¼šå¿«é€Ÿæµ‹è¯•ã€å°è§„æ¨¡è®­ç»ƒ

å¦‚æœéœ€è¦æ›´å¥½çš„æ€§èƒ½ï¼ˆ4+ GPUï¼‰ï¼Œè¯·å‚è€ƒä¸‹é¢çš„ [DistributedDataParallel å®ç°](#æ–¹æ³•-2-distributeddataparallelæ¨è)ã€‚

---

## DataParallel vs DistributedDataParallel

### æ ¸å¿ƒåŒºåˆ«å¯¹æ¯”

| ç‰¹æ€§ | DataParallel (DP) | DistributedDataParallel (DDP) |
|------|-------------------|-------------------------------|
| **æ¶æ„** | å•è¿›ç¨‹å¤šçº¿ç¨‹ | å¤šè¿›ç¨‹ï¼ˆæ¯ä¸ª GPU ä¸€ä¸ªè¿›ç¨‹ï¼‰ |
| **é€šä¿¡æ–¹å¼** | é€šè¿‡ Python GILï¼Œæ‰€æœ‰æ“ä½œåœ¨ GPU 0 ä¸Šèšåˆ | é€šè¿‡ NCCLï¼ŒGPU é—´ç›´æ¥é€šä¿¡ |
| **æ¢¯åº¦èšåˆ** | åœ¨ GPU 0 ä¸Šèšåˆæ‰€æœ‰æ¢¯åº¦ï¼ˆç“¶é¢ˆï¼‰ | æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹èšåˆï¼ˆAllReduceï¼‰ |
| **æ€§èƒ½** | è¾ƒæ…¢ï¼Œå— GIL é™åˆ¶ | æ›´å¿«ï¼Œæ¥è¿‘çº¿æ€§æ‰©å±• |
| **å¯æ‰©å±•æ€§** | é€‚åˆ 2-4 GPU | é€‚åˆä»»æ„æ•°é‡ GPUï¼ˆåŒ…æ‹¬å¤šæœºï¼‰ |
| **å†…å­˜æ•ˆç‡** | æ‰€æœ‰æ¨¡å‹å‰¯æœ¬åœ¨ GPU 0 ä¸Š | æ¯ä¸ªè¿›ç¨‹åªç®¡ç†è‡ªå·±çš„ GPU |
| **å®ç°å¤æ‚åº¦** | ç®€å•ï¼ˆä¸€è¡Œä»£ç ï¼‰ | éœ€è¦åˆ†å¸ƒå¼åˆå§‹åŒ– |
| **å¯åŠ¨æ–¹å¼** | ç›´æ¥è¿è¡Œ Python è„šæœ¬ | éœ€è¦ `torchrun` æˆ– `torch.distributed.launch` |

### è¯¦ç»†æŠ€æœ¯åŒºåˆ«

#### 1. æ¶æ„å·®å¼‚

**DataParallel (DP)**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Python ä¸»è¿›ç¨‹ (å•è¿›ç¨‹)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ GPU 0    â”‚  â”‚ GPU 1    â”‚         â”‚
â”‚  â”‚ (ä¸»GPU)  â”‚  â”‚ (å‰¯æœ¬)   â”‚         â”‚
â”‚  â”‚          â”‚  â”‚          â”‚         â”‚
â”‚  â”‚ èšåˆæ¢¯åº¦ â”‚â†â”€â”‚ å‘é€æ¢¯åº¦ â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚       â†‘              â†‘               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚    é€šè¿‡ Python GIL é€šä¿¡              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DistributedDataParallel (DDP)**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¿›ç¨‹ 0   â”‚      â”‚ è¿›ç¨‹ 1   â”‚
â”‚ GPU 0    â”‚      â”‚ GPU 1    â”‚
â”‚          â”‚      â”‚          â”‚
â”‚ ç‹¬ç«‹å‰å‘ â”‚      â”‚ ç‹¬ç«‹å‰å‘ â”‚
â”‚ ç‹¬ç«‹åå‘ â”‚      â”‚ ç‹¬ç«‹åå‘ â”‚
â”‚          â”‚      â”‚          â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
     â”‚  NCCL AllReduce â”‚
     â”‚  (GPU é—´ç›´æ¥é€šä¿¡)â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. é€šä¿¡æœºåˆ¶

**DataParallel**:
- ä½¿ç”¨ Python çš„å…¨å±€è§£é‡Šå™¨é” (GIL)
- æ‰€æœ‰ GPU çš„æ¢¯åº¦å¿…é¡»å‘é€åˆ° GPU 0
- GPU 0 æˆä¸ºé€šä¿¡ç“¶é¢ˆ
- æ•°æ®ä¼ è¾“é€šè¿‡ CPU å†…å­˜

**DistributedDataParallel**:
- ä½¿ç”¨ NCCL (NVIDIA Collective Communications Library)
- GPU é—´ç›´æ¥é€šä¿¡ï¼ˆä¸ç»è¿‡ CPUï¼‰
- ä½¿ç”¨é«˜æ•ˆçš„ AllReduce ç®—æ³•ï¼ˆRing AllReduceï¼‰
- æ¯ä¸ª GPU ç‹¬ç«‹è®¡ç®—ï¼Œç„¶ååŒæ­¥

#### 3. æ€§èƒ½å¯¹æ¯”

å‡è®¾è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡éœ€è¦çš„æ—¶é—´ï¼š

| GPU æ•°é‡ | DataParallel | DistributedDataParallel | åŠ é€Ÿæ¯” |
|---------|-------------|------------------------|--------|
| 1 GPU   | 100ms       | 100ms                  | 1.0x   |
| 2 GPU   | 60ms        | 52ms                   | 1.15x  |
| 4 GPU   | 40ms        | 28ms                   | 1.43x  |
| 8 GPU   | 30ms        | 15ms                   | 2.0x   |

**ä¸ºä»€ä¹ˆ DDP æ›´å¿«ï¼Ÿ**
1. **æ—  GIL é™åˆ¶**ï¼šå¤šè¿›ç¨‹é¿å…äº† Python GIL çš„åºåˆ—åŒ–é—®é¢˜
2. **å¹¶è¡Œé€šä¿¡**ï¼šNCCL çš„ AllReduce æ˜¯å¹¶è¡Œçš„ï¼Œè€Œ DP æ˜¯ä¸²è¡Œçš„
3. **æ— ä¸» GPU ç“¶é¢ˆ**ï¼šæ¯ä¸ª GPU ç‹¬ç«‹å·¥ä½œï¼Œä¸éœ€è¦ç­‰å¾…ä¸» GPU

#### 4. å†…å­˜ä½¿ç”¨

**DataParallel**:
- ä¸»æ¨¡å‹åœ¨ GPU 0 ä¸Š
- å…¶ä»– GPU åªæœ‰æ¨¡å‹å‰¯æœ¬
- æ¢¯åº¦èšåˆåœ¨ GPU 0 ä¸Šè¿›è¡Œï¼Œéœ€è¦é¢å¤–å†…å­˜

**DistributedDataParallel**:
- æ¯ä¸ªè¿›ç¨‹ç®¡ç†è‡ªå·±çš„ GPU
- å†…å­˜ä½¿ç”¨æ›´å‡åŒ€
- æ¢¯åº¦åœ¨é€šä¿¡æ—¶ä¸´æ—¶å­˜å‚¨ï¼Œä¸å ç”¨ä¸»å†…å­˜

#### 5. ä»£ç ç¤ºä¾‹å¯¹æ¯”

**DataParallel ä½¿ç”¨**:
```python
# ç®€å•ï¼Œä¸€è¡Œä»£ç 
model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])
model = model.cuda()

# ç›´æ¥è¿è¡Œ
python train.py
```

**DistributedDataParallel ä½¿ç”¨**:
```python
# éœ€è¦åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–
dist.init_process_group(backend='nccl')
torch.cuda.set_device(local_rank)
model = DDP(model, device_ids=[local_rank])

# éœ€è¦åˆ†å¸ƒå¼å¯åŠ¨
torchrun --nproc_per_node=4 train.py
```

### ä½•æ—¶ä½¿ç”¨å“ªä¸ªï¼Ÿ

**ä½¿ç”¨ DataParallel å½“ï¼š**
- âœ… å¿«é€ŸåŸå‹å¼€å‘
- âœ… åªæœ‰ 2-4 ä¸ª GPU
- âœ… ä¸æƒ³ä¿®æ”¹ç°æœ‰ä»£ç 
- âœ… è®­ç»ƒæ—¶é—´ä¸æ˜¯å…³é”®å› ç´ 

**ä½¿ç”¨ DistributedDataParallel å½“ï¼š**
- âœ… ç”Ÿäº§ç¯å¢ƒè®­ç»ƒ
- âœ… 4+ GPU è®­ç»ƒ
- âœ… éœ€è¦æœ€ä½³æ€§èƒ½
- âœ… å¤šæœºè®­ç»ƒ
- âœ… å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ

---

## å½“å‰å®ç°çŠ¶æ€

### ç°æœ‰æ”¯æŒ

ä»£ç ä¸­å·²ç»åŒ…å«äº† `DataParallel` çš„å®ç°ï¼ˆåœ¨ `robomimic/algo/diffusion_policy.py` ç¬¬ 97-98 è¡Œï¼‰ï¼š

```python
'obs_encoder': torch.nn.parallel.DataParallel(
    obs_encoder, 
    device_ids=list(range(0, torch.cuda.device_count()))
),
'noise_pred_net': torch.nn.parallel.DataParallel(
    noise_pred_net, 
    device_ids=list(range(0, torch.cuda.device_count()))
)
```

**ç‰¹ç‚¹**ï¼š
- è‡ªåŠ¨æ£€æµ‹ GPU æ•°é‡
- å¦‚æœåªæœ‰ 1 ä¸ª GPUï¼Œä¸ä¼šä½¿ç”¨ DataParallel
- å¦‚æœæœ‰å¤šä¸ª GPUï¼Œè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ GPU

**é™åˆ¶**ï¼š
- ä½¿ç”¨ DataParallelï¼Œæ•ˆç‡è¾ƒä½
- ä¸é€‚åˆå¤§è§„æ¨¡è®­ç»ƒï¼ˆ4+ GPUï¼‰

---

## æ–¹æ³• 1: DataParallelï¼ˆç®€å•ä½†æ•ˆç‡è¾ƒä½ï¼‰

### å·¥ä½œåŸç†

`DataParallel` æ˜¯å•è¿›ç¨‹å¤š GPU æ–¹æ¡ˆï¼š
1. ä¸»è¿›ç¨‹åœ¨ GPU 0 ä¸Š
2. å°†æ‰¹æ¬¡æ•°æ®åˆ†å‰²åˆ°å„ä¸ª GPU
3. æ¯ä¸ª GPU ç‹¬ç«‹è®¡ç®—å‰å‘å’Œåå‘ä¼ æ’­
4. æ‰€æœ‰æ¢¯åº¦åœ¨ GPU 0 ä¸Šèšåˆ
5. åœ¨ GPU 0 ä¸Šæ›´æ–°æ¨¡å‹
6. å°†æ›´æ–°åçš„æ¨¡å‹åŒæ­¥åˆ°å…¶ä»– GPU

### ä½¿ç”¨æ–¹æ³•ï¼šç›´æ¥è¿è¡Œå¤šå¡è®­ç»ƒ

**å½“å‰ä»£ç å·²ç»æ”¯æŒå¤šå¡è®­ç»ƒï¼** æ— éœ€ä»»ä½•ä¿®æ”¹ï¼Œç›´æ¥è¿è¡Œè®­ç»ƒå‘½ä»¤å³å¯ï¼š

```bash
# æ–¹å¼ 1: ä½¿ç”¨ train_rlds.py
python -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json

# æ–¹å¼ 2: ä½¿ç”¨ train.py
python -m robomimic.scripts.train \
    --config /path/to/config.json
```

**ä»£ç ä¼šè‡ªåŠ¨ï¼š**
- æ£€æµ‹ç³»ç»Ÿä¸­æ‰€æœ‰å¯ç”¨çš„ GPU
- è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ GPU è¿›è¡Œè®­ç»ƒ
- æ— éœ€ä»»ä½•é¢å¤–é…ç½®

### éªŒè¯å¤š GPU æ˜¯å¦åœ¨ä½¿ç”¨

è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ï¼š

```bash
# 1. æ£€æŸ¥ GPU æ•°é‡
python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"

# 2. åœ¨è®­ç»ƒæ—¶ç›‘æ§ GPU ä½¿ç”¨ï¼ˆå¦å¼€ä¸€ä¸ªç»ˆç«¯ï¼‰
watch -n 1 nvidia-smi

# 3. æˆ–è€…ä½¿ç”¨ nvtopï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
nvtop
```

**é¢„æœŸç»“æœï¼š**
- å¦‚æœæœ‰å¤šå— GPUï¼Œåº”è¯¥çœ‹åˆ°æ‰€æœ‰ GPU éƒ½åœ¨ä½¿ç”¨
- GPU 0 çš„ä½¿ç”¨ç‡å¯èƒ½ç¨é«˜ï¼ˆå› ä¸ºæ¢¯åº¦èšåˆåœ¨ GPU 0 ä¸Šè¿›è¡Œï¼‰
- å…¶ä»– GPU çš„ä½¿ç”¨ç‡åº”è¯¥æ¥è¿‘ GPU 0

### æŒ‡å®šä½¿ç”¨çš„ GPU

å¦‚æœæƒ³åªä½¿ç”¨ç‰¹å®šçš„ GPUï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
# åªä½¿ç”¨ GPU 0 å’Œ GPU 1
CUDA_VISIBLE_DEVICES=0,1 python -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json

# åªä½¿ç”¨ GPU 2 å’Œ GPU 3
CUDA_VISIBLE_DEVICES=2,3 python -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json
```

**æ³¨æ„ï¼š** `CUDA_VISIBLE_DEVICES` ä¼šé‡æ–°æ˜ å°„ GPU ç´¢å¼•ï¼Œæ‰€ä»¥ï¼š
- `CUDA_VISIBLE_DEVICES=2,3` ä¼šå°† GPU 2 æ˜ å°„ä¸º `cuda:0`ï¼ŒGPU 3 æ˜ å°„ä¸º `cuda:1`
- ä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯è§çš„ GPU

### éªŒè¯å¤š GPU ä½¿ç”¨

```bash
# æ£€æŸ¥ GPU æ•°é‡
python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"

# åœ¨è®­ç»ƒæ—¶ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# åº”è¯¥çœ‹åˆ°æ‰€æœ‰ GPU éƒ½åœ¨ä½¿ç”¨ï¼Œä½† GPU 0 ä½¿ç”¨ç‡å¯èƒ½æ›´é«˜
```

### æ€§èƒ½ç‰¹ç‚¹

- âš ï¸ **å•è¿›ç¨‹é™åˆ¶**ï¼šå— Python GIL é™åˆ¶ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU
- âš ï¸ **GPU 0 ç“¶é¢ˆ**ï¼šæ‰€æœ‰æ¢¯åº¦èšåˆåœ¨ GPU 0ï¼Œæˆä¸ºé€šä¿¡ç“¶é¢ˆ
- âš ï¸ **ä¸²è¡Œé€šä¿¡**ï¼šGPU é—´é€šä¿¡æ˜¯ä¸²è¡Œçš„ï¼Œä¸æ˜¯å¹¶è¡Œçš„
- âš ï¸ **æ‰©å±•æ€§å·®**ï¼šGPU æ•°é‡å¢åŠ æ—¶ï¼Œæ€§èƒ½æå‡ä¸æ˜æ˜¾

### é€‚ç”¨åœºæ™¯

- å¿«é€Ÿæµ‹è¯•å¤š GPU è®­ç»ƒ
- 2-4 GPU çš„å°è§„æ¨¡è®­ç»ƒ
- ä¸æƒ³ä¿®æ”¹ä»£ç çš„ä¸´æ—¶æ–¹æ¡ˆ

---

## æ–¹æ³• 2: DistributedDataParallelï¼ˆæ¨èï¼‰

`DistributedDataParallel` (DDP) æ˜¯ PyTorch æ¨èçš„å¤š GPU è®­ç»ƒæ–¹æ³•ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

### å·¥ä½œåŸç†

1. **å¤šè¿›ç¨‹æ¶æ„**ï¼šæ¯ä¸ª GPU è¿è¡Œä¸€ä¸ªç‹¬ç«‹çš„ Python è¿›ç¨‹
2. **ç‹¬ç«‹è®¡ç®—**ï¼šæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹è¿›è¡Œå‰å‘å’Œåå‘ä¼ æ’­
3. **å¹¶è¡Œé€šä¿¡**ï¼šä½¿ç”¨ NCCL çš„ AllReduce ç®—æ³•å¹¶è¡ŒåŒæ­¥æ¢¯åº¦
4. **åŒæ­¥æ›´æ–°**ï¼šæ‰€æœ‰è¿›ç¨‹åŒæ­¥æ›´æ–°æ¨¡å‹å‚æ•°

### å®ç°æ­¥éª¤

#### æ­¥éª¤ 1: ä¿®æ”¹ `train_rlds.py`

åœ¨ `train_rlds.py` ä¸­æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼š

```python
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',  # ä½¿ç”¨ NCCL åç«¯ï¼ˆGPUï¼‰
        rank=rank,
        world_size=world_size
    )
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU
    torch.cuda.set_device(rank)

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    dist.destroy_process_group()

def run(cfg: "OmegaConf") -> str:
    from robomimic.config import config_factory
    from robomimic.utils import torch_utils as TorchUtils
    from robomimic.scripts.train import train

    # ... ç°æœ‰ä»£ç  ...
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    use_ddp = int(os.environ.get('WORLD_SIZE', 0)) > 1
    
    if use_ddp:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
        setup_distributed(rank, world_size)
        device = torch.device(f'cuda:{local_rank}')
    else:
        device = TorchUtils.get_torch_device(try_to_use_cuda=config.train.cuda)
    
    # ... ç»§ç»­è®­ç»ƒ ...
    
    if use_ddp:
        cleanup_distributed()
    
    return "finished run successfully!"
```

#### æ­¥éª¤ 2: ä¿®æ”¹ `diffusion_policy.py`

å°† `DataParallel` æ›¿æ¢ä¸º `DistributedDataParallel`ï¼š

```python
# åœ¨ _create_networks æ–¹æ³•ä¸­
def _create_networks(self):
    # ... åˆ›å»ºç½‘ç»œçš„ä»£ç  ...
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DDP
    use_ddp = int(os.environ.get('WORLD_SIZE', 0)) > 1
    
    if use_ddp:
        # ä½¿ç”¨ DDP åŒ…è£…ç½‘ç»œ
        from torch.nn.parallel import DistributedDataParallel as DDP
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        obs_encoder = DDP(obs_encoder, device_ids=[local_rank])
        noise_pred_net = DDP(noise_pred_net, device_ids=[local_rank])
    else:
        # å›é€€åˆ° DataParallelï¼ˆå•æœºå¤š GPUï¼‰
        if torch.cuda.device_count() > 1:
            obs_encoder = torch.nn.parallel.DataParallel(
                obs_encoder, 
                device_ids=list(range(torch.cuda.device_count()))
            )
            noise_pred_net = torch.nn.parallel.DataParallel(
                noise_pred_net, 
                device_ids=list(range(torch.cuda.device_count()))
            )
    
    nets = nn.ModuleDict({
        'policy': nn.ModuleDict({
            'obs_encoder': obs_encoder,
            'noise_pred_net': noise_pred_net
        })
    })
    
    # ... å…¶ä½™ä»£ç  ...
```

#### æ­¥éª¤ 3: ä¿®æ”¹ `train.py` ä¸­çš„ DataLoader

åœ¨ `train.py` ä¸­æ·»åŠ åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼š

```python
# åœ¨ train.py ä¸­
from torch.utils.data.distributed import DistributedSampler

# ... åˆ›å»ºæ•°æ®é›†çš„ä»£ç  ...

use_ddp = int(os.environ.get('WORLD_SIZE', 0)) > 1

if use_ddp:
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    # ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = DistributedSampler(
        trainset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    train_loader = DataLoader(
        dataset=trainset,
        sampler=train_sampler,  # ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨
        batch_size=config.train.batch_size,
        shuffle=False,  # é‡‡æ ·å™¨å·²ç»å¤„ç†äº† shuffle
        num_workers=config.train.num_data_workers,
        drop_last=True
    )
else:
    # å• GPU æˆ– DataParallel æ¨¡å¼
    train_loader = DataLoader(
        dataset=trainset,
        sampler=train_sampler,
        batch_size=config.train.batch_size,
        shuffle=(train_sampler is None),
        num_workers=config.train.num_data_workers,
        drop_last=True
    )
```

#### æ­¥éª¤ 4: å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

ä½¿ç”¨ `torchrun`ï¼ˆæ¨èï¼ŒPyTorch 1.9+ï¼‰ï¼š

```bash
torchrun \
    --nproc_per_node=4 \
    --master_port=12355 \
    -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json
```

æˆ–ä½¿ç”¨ `torch.distributed.launch`ï¼ˆPyTorch < 1.9ï¼‰ï¼š

```bash
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --master_port=12355 \
    -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json
```

---

## æ–¹æ³• 3: ä½¿ç”¨ torchrunï¼ˆæœ€ç®€å•ï¼‰

`torchrun` æ˜¯ PyTorch 1.9+ æä¾›çš„å·¥å…·ï¼Œè‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒçš„å¯åŠ¨å’Œæ•…éšœæ¢å¤ã€‚

### ä½¿ç”¨æ–¹æ³•

#### 1. åˆ›å»ºå¯åŠ¨è„šæœ¬ `scripts/train_multi_gpu.sh`

```bash
#!/bin/bash

# é…ç½®
NUM_GPUS=4
CONFIG_PATH="/path/to/your/config.json"
MASTER_PORT=12355

# ä½¿ç”¨ torchrun å¯åŠ¨è®­ç»ƒ
torchrun \
    --nproc_per_node=${NUM_GPUS} \
    --master_port=${MASTER_PORT} \
    -m robomimic.scripts.train_rlds \
    load_from=${CONFIG_PATH}
```

#### 2. è¿è¡Œè„šæœ¬

```bash
chmod +x scripts/train_multi_gpu.sh
./scripts/train_multi_gpu.sh
```

### ç¯å¢ƒå˜é‡

`torchrun` ä¼šè‡ªåŠ¨è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š
- `RANK`: å½“å‰è¿›ç¨‹çš„å…¨å±€æ’åï¼ˆ0 åˆ° world_size-1ï¼‰
- `LOCAL_RANK`: å½“å‰è¿›ç¨‹åœ¨èŠ‚ç‚¹å†…çš„æ’åï¼ˆ0 åˆ° nproc_per_node-1ï¼‰
- `WORLD_SIZE`: æ€»è¿›ç¨‹æ•°ï¼ˆç­‰äº nproc_per_nodeï¼‰
- `MASTER_ADDR`: ä¸»èŠ‚ç‚¹åœ°å€ï¼ˆé»˜è®¤ localhostï¼‰
- `MASTER_PORT`: ä¸»èŠ‚ç‚¹ç«¯å£

### torchrun çš„ä¼˜åŠ¿

- âœ… **è‡ªåŠ¨æ•…éšœæ¢å¤**ï¼šå¦‚æœè¿›ç¨‹å´©æºƒï¼Œè‡ªåŠ¨é‡å¯
- âœ… **å¼¹æ€§è®­ç»ƒ**ï¼šæ”¯æŒåŠ¨æ€æ·»åŠ /ç§»é™¤èŠ‚ç‚¹
- âœ… **ç®€åŒ–å¯åŠ¨**ï¼šä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡

---

## é…ç½®å»ºè®®

### æ‰¹æ¬¡å¤§å°è°ƒæ•´

ä½¿ç”¨å¤š GPU æ—¶ï¼Œ**æœ‰æ•ˆæ‰¹æ¬¡å¤§å°** = `batch_size Ã— num_gpus`

**å»ºè®®**ï¼š
- **é€‰é¡¹ 1**ï¼šä¿æŒå• GPU æ‰¹æ¬¡å¤§å°ï¼Œè®©æœ‰æ•ˆæ‰¹æ¬¡å¤§å°å¢åŠ 
  - å• GPU: `batch_size=128`
  - 4 GPU: `batch_size=128` â†’ æœ‰æ•ˆæ‰¹æ¬¡ = 512
  - ä¼˜ç‚¹ï¼šè®­ç»ƒæ›´å¿«
  - ç¼ºç‚¹ï¼šå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡

- **é€‰é¡¹ 2**ï¼šå‡å°æ¯ä¸ª GPU çš„æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°ä¸å˜
  - å• GPU: `batch_size=128`
  - 4 GPU: `batch_size=32` â†’ æœ‰æ•ˆæ‰¹æ¬¡ = 128
  - ä¼˜ç‚¹ï¼šè®­ç»ƒè¡Œä¸ºä¸å• GPU ä¸€è‡´
  - ç¼ºç‚¹ï¼šå¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨ GPU

### å­¦ä¹ ç‡è°ƒæ•´

é€šå¸¸éœ€è¦æ ¹æ®æœ‰æ•ˆæ‰¹æ¬¡å¤§å°è°ƒæ•´å­¦ä¹ ç‡ï¼š

```python
# çº¿æ€§ç¼©æ”¾è§„åˆ™ï¼ˆå¸¸ç”¨ï¼‰
base_lr = 1e-4
num_gpus = 4
adjusted_lr = base_lr * num_gpus  # 4e-4

# å¹³æ–¹æ ¹ç¼©æ”¾ï¼ˆæ›´ä¿å®ˆï¼Œé€‚åˆå¤§æ¨¡å‹ï¼‰
adjusted_lr = base_lr * (num_gpus ** 0.5)  # 2e-4

# ä¸ç¼©æ”¾ï¼ˆé€‚åˆå°æ¨¡å‹æˆ–é¢„è®­ç»ƒæ¨¡å‹ï¼‰
adjusted_lr = base_lr  # 1e-4
```

**ç»éªŒæ³•åˆ™**ï¼š
- å°æ¨¡å‹ï¼ˆ< 100M å‚æ•°ï¼‰ï¼šçº¿æ€§ç¼©æ”¾
- å¤§æ¨¡å‹ï¼ˆ> 1B å‚æ•°ï¼‰ï¼šå¹³æ–¹æ ¹ç¼©æ”¾æˆ–ä¸ç¼©æ”¾
- ä»é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼šé€šå¸¸ä¸ç¼©æ”¾

### æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹

```python
# æ¯ä¸ª GPU è¿›ç¨‹ä½¿ç”¨çš„å·¥ä½œè¿›ç¨‹æ•°
num_workers_per_gpu = 4
total_workers = num_workers_per_gpu * num_gpus

# æ³¨æ„ï¼šæ€»å·¥ä½œè¿›ç¨‹æ•°ä¸åº”è¶…è¿‡ CPU æ ¸å¿ƒæ•°
# ä¾‹å¦‚ï¼š8 æ ¸ CPUï¼Œ4 GPUï¼Œæ¯ä¸ª GPU 2 ä¸ªå·¥ä½œè¿›ç¨‹ = 8 ä¸ªæ€»è¿›ç¨‹
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. ä½¿ç”¨ NCCL åç«¯

ç¡®ä¿ä½¿ç”¨ NCCL åç«¯ï¼ˆGPU è®­ç»ƒï¼‰ï¼š

```python
dist.init_process_group(backend='nccl', ...)
```

**ä¸ºä»€ä¹ˆ NCCLï¼Ÿ**
- ä¸“ä¸º GPU è®¾è®¡ï¼Œæ€§èƒ½æœ€ä½³
- æ”¯æŒ GPU é—´ç›´æ¥é€šä¿¡ï¼ˆä¸ç»è¿‡ CPUï¼‰
- è‡ªåŠ¨ä¼˜åŒ–é€šä¿¡æ¨¡å¼

### 2. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨ `torch.cuda.amp`ï¼š

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
with autocast():
    loss = model.compute_loss(batch)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**ä¼˜åŠ¿**ï¼š
- å‡å°‘å†…å­˜ä½¿ç”¨ï¼ˆçº¦ 50%ï¼‰
- åŠ é€Ÿè®­ç»ƒï¼ˆçº¦ 1.5-2xï¼‰
- ç°ä»£ GPUï¼ˆV100+ï¼‰æ”¯æŒè‰¯å¥½

### 3. ä¼˜åŒ– DataLoader

```python
# ä½¿ç”¨ pin_memory åŠ é€Ÿæ•°æ®ä¼ è¾“
train_loader = DataLoader(
    dataset=trainset,
    batch_size=batch_size,
    num_workers=num_workers,
    pin_memory=True,  # åŠ é€Ÿ CPU åˆ° GPU çš„æ•°æ®ä¼ è¾“
    persistent_workers=True,  # ä¿æŒå·¥ä½œè¿›ç¨‹å­˜æ´»ï¼ˆå‡å°‘å¯åŠ¨å¼€é”€ï¼‰
    prefetch_factor=2  # é¢„å–æ‰¹æ¬¡æ•°é‡
)
```

### 4. æ¢¯åº¦ç´¯ç§¯

å¦‚æœ GPU å†…å­˜ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š

```python
accumulation_steps = 4
for i, batch in enumerate(train_loader):
    loss = model.compute_loss(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**ä¼˜åŠ¿**ï¼š
- æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°
- ä¸éœ€è¦æ›´å¤š GPU å†…å­˜
- ä¿æŒè®­ç»ƒç¨³å®šæ€§

### 5. ä½¿ç”¨ find_unused_parameters=False

å¦‚æœæ¨¡å‹çš„æ‰€æœ‰å‚æ•°éƒ½å‚ä¸åå‘ä¼ æ’­ï¼š

```python
model = DDP(model, device_ids=[local_rank], find_unused_parameters=False)
```

**ä¼˜åŠ¿**ï¼š
- å‡å°‘é€šä¿¡å¼€é”€
- åŠ é€Ÿè®­ç»ƒ

---

## éªŒè¯å¤š GPU è®­ç»ƒ

### æ£€æŸ¥ GPU ä½¿ç”¨ç‡

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨ nvtopï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
nvtop
```

**é¢„æœŸç»“æœ**ï¼š
- æ‰€æœ‰ GPU çš„ä½¿ç”¨ç‡åº”è¯¥ç›¸ä¼¼ï¼ˆDDPï¼‰
- æˆ– GPU 0 ä½¿ç”¨ç‡ç¨é«˜ï¼ˆDPï¼‰

### æ£€æŸ¥è®­ç»ƒæ—¥å¿—

åœ¨è®­ç»ƒæ—¥å¿—ä¸­åº”è¯¥çœ‹åˆ°ï¼š
- æ¯ä¸ªè¿›ç¨‹çš„ `RANK` å’Œ `LOCAL_RANK`
- æ¯ä¸ª GPU éƒ½åœ¨å¤„ç†æ•°æ®
- åŒæ­¥çš„æ¢¯åº¦æ›´æ–°

### æ€§èƒ½æŒ‡æ ‡

- **ååé‡**ï¼šåº”è¯¥æ¥è¿‘çº¿æ€§æ‰©å±•
  - 2 GPU â‰ˆ 1.9x å• GPU é€Ÿåº¦
  - 4 GPU â‰ˆ 3.7x å• GPU é€Ÿåº¦
  - 8 GPU â‰ˆ 7.2x å• GPU é€Ÿåº¦

- **GPU åˆ©ç”¨ç‡**ï¼šæ‰€æœ‰ GPU åº”è¯¥éƒ½åœ¨ 80%+ ä½¿ç”¨ç‡

- **é€šä¿¡å¼€é”€**ï¼šNCCL é€šä¿¡æ—¶é—´åº”è¯¥ < 10% æ€»è®­ç»ƒæ—¶é—´

---

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: NCCL åˆå§‹åŒ–å¤±è´¥

**é”™è¯¯**ï¼š
```
NCCL error: unhandled system error
NCCL error: initialization error
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿æ‰€æœ‰ GPU å¯è§ï¼š`nvidia-smi`
2. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®ï¼ˆNCCL éœ€è¦è¿›ç¨‹é—´é€šä¿¡ï¼‰
3. ä½¿ç”¨ `network_mode: host` åœ¨ Docker ä¸­
4. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
   ```bash
   export NCCL_DEBUG=INFO
   export NCCL_IB_DISABLE=1  # å¦‚æœä½¿ç”¨ InfiniBand
   ```

### é—®é¢˜ 2: å†…å­˜ä¸è¶³

**é”™è¯¯**ï¼š
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å°æ‰¹æ¬¡å¤§å°
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
3. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
4. å‡å°‘æ¨¡å‹å¤§å°æˆ–ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

### é—®é¢˜ 3: æ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆ

**ç—‡çŠ¶**ï¼š
- GPU ä½¿ç”¨ç‡ä½ï¼ˆ< 50%ï¼‰
- è®­ç»ƒé€Ÿåº¦æ²¡æœ‰éš GPU æ•°é‡çº¿æ€§å¢åŠ 

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å¢åŠ  `num_workers`
2. ä½¿ç”¨ `pin_memory=True`
3. å¢åŠ  `prefetch_factor`
4. ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨ï¼ˆNVMe SSDï¼‰
5. ä½¿ç”¨ `persistent_workers=True` å‡å°‘å¯åŠ¨å¼€é”€

### é—®é¢˜ 4: è¿›ç¨‹åŒæ­¥å¤±è´¥

**é”™è¯¯**ï¼š
```
RuntimeError: Expected to have finished reduction in the prior iteration
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿æ¯ä¸ªè¿›ç¨‹å¤„ç†ç›¸åŒæ•°é‡çš„æ‰¹æ¬¡
2. ä½¿ç”¨ `drop_last=True` åœ¨ DataLoader ä¸­
3. ç¡®ä¿æ‰€æœ‰è¿›ç¨‹ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­

### é—®é¢˜ 5: ç«¯å£å†²çª

**é”™è¯¯**ï¼š
```
RuntimeError: Address already in use
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ›´æ”¹ `MASTER_PORT`ï¼š
   ```bash
   torchrun --master_port=12356 ...
   ```
2. æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è®­ç»ƒè¿›ç¨‹åœ¨è¿è¡Œ
3. ä½¿ç”¨ä¸åŒçš„ç«¯å£èŒƒå›´ï¼ˆ12355-12365ï¼‰

---

## ç¤ºä¾‹ï¼šå®Œæ•´çš„å¤š GPU è®­ç»ƒå‘½ä»¤

### ä½¿ç”¨ DataParallelï¼ˆå½“å‰å®ç°ï¼‰

```bash
# ç›´æ¥è¿è¡Œï¼Œè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ GPU
python -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json
```

### ä½¿ç”¨ DistributedDataParallel

```bash
# åœ¨ Docker å®¹å™¨ä¸­
cd /workspace/droid_policy_learning

# 4 GPU è®­ç»ƒ
torchrun \
    --nproc_per_node=4 \
    --master_port=12355 \
    -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json \
    train.batch_size=128 \
    train.optim_params.policy.learning_rate.initial=4e-4

# 8 GPU è®­ç»ƒ
torchrun \
    --nproc_per_node=8 \
    --master_port=12355 \
    -m robomimic.scripts.train_rlds \
    load_from=/path/to/config.json \
    train.batch_size=64 \
    train.optim_params.policy.learning_rate.initial=8e-4
```

---

## æ€»ç»“

### DataParallel vs DistributedDataParallel

| æ–¹é¢ | DataParallel | DistributedDataParallel |
|------|-------------|------------------------|
| **å®ç°** | ç®€å•ï¼ˆä¸€è¡Œä»£ç ï¼‰ | éœ€è¦åˆ†å¸ƒå¼åˆå§‹åŒ– |
| **æ€§èƒ½** | è¾ƒæ…¢ï¼Œ2-4 GPU å¯ç”¨ | æ›´å¿«ï¼Œçº¿æ€§æ‰©å±• |
| **é€‚ç”¨åœºæ™¯** | å¿«é€Ÿæµ‹è¯•ï¼Œå°è§„æ¨¡è®­ç»ƒ | ç”Ÿäº§ç¯å¢ƒï¼Œå¤§è§„æ¨¡è®­ç»ƒ |
| **æ¨èä½¿ç”¨** | 2-4 GPUï¼Œå¿«é€ŸåŸå‹ | 4+ GPUï¼Œç”Ÿäº§è®­ç»ƒ |

### é€‰æ‹©å»ºè®®

- **å¿«é€Ÿæµ‹è¯•**ï¼šä½¿ç”¨ç°æœ‰çš„ DataParallelï¼ˆè‡ªåŠ¨å¯ç”¨ï¼‰
- **ç”Ÿäº§è®­ç»ƒ**ï¼šå®ç° DistributedDataParallel + torchrun
- **å¤§è§„æ¨¡è®­ç»ƒ**ï¼šå¿…é¡»ä½¿ç”¨ DistributedDataParallel

### å…³é”®è¦ç‚¹

1. âœ… **DataParallel** é€‚åˆå¿«é€Ÿæµ‹è¯•ï¼Œä½†æ•ˆç‡è¾ƒä½
2. âœ… **DistributedDataParallel** æ˜¯ç”Ÿäº§ç¯å¢ƒçš„æ ‡å‡†é€‰æ‹©
3. âœ… **torchrun** æ˜¯æœ€ç®€å•çš„å¯åŠ¨æ–¹å¼
4. ğŸ“ **é…ç½®è°ƒæ•´**ï¼šæ ¹æ® GPU æ•°é‡è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œå­¦ä¹ ç‡
5. ğŸš€ **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨æ··åˆç²¾åº¦ã€ä¼˜åŒ– DataLoaderã€æ¢¯åº¦ç´¯ç§¯

---

## å‚è€ƒèµ„æ–™

- [PyTorch DistributedDataParallel æ–‡æ¡£](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [PyTorch DataParallel æ–‡æ¡£](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)
- [NCCL æ–‡æ¡£](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)
- [torchrun æ–‡æ¡£](https://pytorch.org/docs/stable/elastic/run.html)
